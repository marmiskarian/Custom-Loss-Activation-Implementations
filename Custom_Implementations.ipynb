{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FWMS19abYkY",
    "tags": []
   },
   "source": [
    "# Implementing and analyzing Custom Loss Functions in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkOwunLxfbyG"
   },
   "source": [
    "This task sequence introduces the development of custom loss functions in PyTorch, with a focus on applying theoretical knowledge to practical implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2CVdpJMOEhn",
    "outputId": "758bf6e1-43f4-451d-eef2-dcf11f18bc2d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: 0.375\n",
      "Gradients on y_pred: tensor([-0.2500, -0.2500,  0.0000, -0.2500])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class L1Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    L1 Loss, also known as Mean Absolute Error (MAE).\n",
    "    \"\"\"\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward pass for L1 loss using PyTorch operations.\n",
    "\n",
    "        :param y_pred: Predicted values (Tensor).\n",
    "        :param y_true: Ground truth values (Tensor).\n",
    "        :return: Scalar tensor representing the L1 loss.\n",
    "        \"\"\"\n",
    "        return torch.mean(torch.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define sample predicted values and ground truth values for testing the implementation\n",
    "    y_pred = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "    y_true = torch.tensor([1.5, 2.5, 3.0, 4.5])\n",
    "\n",
    "    # Initialize custom L1Loss\n",
    "    criterion = L1Loss()\n",
    "\n",
    "    # Compute the loss using L1Loss class and print it\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    print(f\"L1 Loss: {loss}\")\n",
    "\n",
    "    # Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd)\n",
    "    loss.backward()\n",
    "    print(f\"Gradients on y_pred: {y_pred.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9mYQ1IeTYUHv",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Loss: 0.1875\n",
      "Gradients on y_pred: tensor([-0.2500, -0.2500,  0.0000, -0.2500])\n"
     ]
    }
   ],
   "source": [
    "class L2Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    L2 Loss, also known as Mean Squared Error (MSE).\n",
    "    \"\"\"\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward pass for L2 loss using PyTorch operations.\n",
    "        :param y_pred: Predicted values (Tensor).\n",
    "        :param y_true: Ground truth values (Tensor).\n",
    "        :return: Scalar tensor representing the L2 loss.\n",
    "        \"\"\"\n",
    "        return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define sample predicted values and ground truth values for testing the implementation\n",
    "    # Ensure y_pred and y_true are PyTorch tensors\n",
    "    y_pred = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "    y_true = torch.tensor([1.5, 2.5, 3.0, 4.5])\n",
    "\n",
    "    # Initialize custom L1Loss\n",
    "    criterion = L2Loss()\n",
    "\n",
    "    # Compute the loss using L2Loss class and print it\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    print(f\"L2 Loss: {loss}\")\n",
    "\n",
    "    # Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd)\n",
    "    loss.backward()\n",
    "    print(f\"Gradients on y_pred: {y_pred.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qyonhh7xOHNN",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE Loss: 0.16425204277038574\n",
      "Gradients on y_pred: tensor([-0.2778,  0.3125, -0.3125,  0.2778])\n"
     ]
    }
   ],
   "source": [
    "class BCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy (BCE) Loss implemented for PyTorch.\n",
    "    Note: PyTorch already provides nn.BCELoss, but implementing it manually can be educational.\n",
    "    \"\"\"\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward pass for BCE loss using PyTorch operations.\n",
    "\n",
    "        :param y_pred: Predicted probabilities (Tensor) with values in range [0, 1].\n",
    "        :param y_true: Ground truth values (Tensor) with binary values 0 or 1.\n",
    "        :return: Scalar tensor representing the BCE loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-12  # Small value to avoid numerical instability for log(0)\n",
    "        bce_loss = -torch.mean(y_true * torch.log(torch.clamp(y_pred, epsilon, 1.0)) + (1 - y_true) * torch.log(torch.clamp(1 - y_pred, epsilon, 1.0)))\n",
    "        return bce_loss\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define sample predicted values and ground truth values for testing the implementation\n",
    "    # Ensure y_pred and y_true are PyTorch tensors\n",
    "    y_pred = torch.tensor([0.9, 0.2, 0.8, 0.1], requires_grad=True)\n",
    "    y_true = torch.tensor([1, 0, 1, 0], dtype=torch.float32)\n",
    "\n",
    "    # Initialize custom BCELoss\n",
    "    criterion = BCELoss()\n",
    "\n",
    "    # Compute the loss using BCELoss class and print it\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    print(f\"BCE Loss: {loss}\")\n",
    "\n",
    "    # Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd)\n",
    "    loss.backward()\n",
    "    print(f\"Gradients on y_pred: {y_pred.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CagRBaWXag58",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE Loss: 0.8266606330871582\n",
      "Gradients on y_pred: tensor([[-0.2031,  0.1066,  0.0965],\n",
      "        [ 0.0830, -0.1661,  0.0830],\n",
      "        [ 0.0955,  0.0955, -0.1909]])\n"
     ]
    }
   ],
   "source": [
    "class CELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the Cross-Entropy Loss for multi-class classification in PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CELoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Forward pass for Cross-Entropy loss.\n",
    "\n",
    "        :param logits: Logits from the model (Tensor). Shape: [batch_size, num_classes].\n",
    "        :param targets: Ground truth class indices (Tensor). Shape: [batch_size].\n",
    "        :return: Scalar tensor representing the CE loss.\n",
    "        \"\"\"\n",
    "        batch_size = logits.size(0)\n",
    "        log_softmax = logits - torch.log(torch.sum(torch.exp(logits), dim=1, keepdim=True))\n",
    "        ce_loss = -torch.sum(log_softmax[range(batch_size), targets]) / batch_size\n",
    "        return ce_loss\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define sample predicted values and ground truth values for testing the implementation\n",
    "    # Ensure y_pred and y_true are PyTorch tensors\n",
    "    y_pred = torch.tensor([[0.5, 0.3, 0.2], [0.1, 0.8, 0.1], [0.2, 0.2, 0.6]], requires_grad=True)\n",
    "    y_true = torch.tensor([0, 1, 2])  # Assuming the ground truth class indices\n",
    "\n",
    "    # Initialize custom CELoss\n",
    "    criterion = CELoss()\n",
    "\n",
    "    # Compute the loss using BCELoss class and print it\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    print(f\"CE Loss: {loss}\")\n",
    "\n",
    "    # Perform a backward pass to compute gradients (optional demonstration of PyTorch's autograd)\n",
    "    loss.backward()\n",
    "    print(f\"Gradients on y_pred: {y_pred.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dttnSzVVdL7P",
    "tags": []
   },
   "source": [
    "# Implementing Custom Activation Functions in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHCxEDGyv4w0"
   },
   "source": [
    "Note: The backward calculation for the Softmax function is not straightforward; hence, we may rely solely on PyTorch's built-in functionality for the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Krp6yaXKddz_",
    "outputId": "2fcbd3c4-a2ec-420a-db3b-7a72bb4c8d3c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom ReLU output: tensor([0., 0., 1., 2.], grad_fn=<MaximumBackward0>)\n",
      "Custom ReLU gradients: tensor([0., 0., 1., 1.], grad_fn=<IndexPutBackward0>)\n",
      "PyTorch ReLU output: tensor([0., 0., 1., 2.], grad_fn=<ReluBackward0>)\n",
      "PyTorch ReLU gradients: tensor([0., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "class ReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the ReLU activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for ReLU.\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor where ReLU(x) = max(0, x).\n",
    "        \"\"\"\n",
    "        return torch.maximum(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for custom ReLU.\n",
    "        :param grad_output: Gradient tensor of the output.\n",
    "        :return: Gradient tensor for the input.\n",
    "        \"\"\"\n",
    "        # Gradient of ReLU is 1 for input > 0; otherwise, it's 0\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[grad_input >= 1] = 1\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a sample input tensor\n",
    "    x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "\n",
    "    # Initialize the custom ReLU activation function\n",
    "    custom_relu = ReLU()\n",
    "\n",
    "    # Compute the activation using the custom ReLU class\n",
    "    activated_x_custom = custom_relu(x)\n",
    "\n",
    "    # Perform a backward pass to compute gradients for the custom implementation\n",
    "    gradients_custom = custom_relu.backward(activated_x_custom)\n",
    "\n",
    "    # Print the outputs and gradients from the custom implementation\n",
    "    print(\"Custom ReLU output:\", activated_x_custom)\n",
    "    print(\"Custom ReLU gradients:\", gradients_custom)\n",
    "\n",
    "    # Reset gradients to zero before another backward pass\n",
    "    x.grad = None\n",
    "\n",
    "    # Compute the activation using PyTorch's built-in relu function\n",
    "    activated_x_torch = torch.relu(x)\n",
    "\n",
    "    # Perform a backward pass to compute gradients for PyTorch's implementation\n",
    "    activated_x_torch.backward(torch.ones_like(x))\n",
    "    gradients_torch = x.grad\n",
    "\n",
    "    # Print the outputs and gradients from PyTorch's implementation\n",
    "    print(\"PyTorch ReLU output:\", activated_x_torch)\n",
    "    print(\"PyTorch ReLU gradients:\", gradients_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gJWfdJJKdeoV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Sigmoid output: tensor([0.2689, 0.5000, 0.7311, 0.8808], grad_fn=<MulBackward0>)\n",
      "Custom Sigmoid gradients: tensor([0.1966, 0.2500, 0.1966, 0.1050], grad_fn=<MulBackward0>)\n",
      "PyTorch Sigmoid output: tensor([0.2689, 0.5000, 0.7311, 0.8808], grad_fn=<SigmoidBackward0>)\n",
      "PyTorch Sigmoid gradients: tensor([0.1966, 0.2500, 0.1966, 0.1050])\n"
     ]
    }
   ],
   "source": [
    "class Sigmoid(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Sigmoid.\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor where Sigmoid(x) = 1 / (1 + exp(-x)).\n",
    "        \"\"\"\n",
    "        self.output = 1 / (1 + torch.exp(-x))\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for custom Sigmoid.\n",
    "        :param grad_output: Gradient tensor of the output.\n",
    "        :return: Gradient tensor for the input.\n",
    "        \"\"\"\n",
    "        sigmoid_grad = self.output * (1 - self.output) * grad_output\n",
    "        return sigmoid_grad\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a sample input tensor\n",
    "    x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "\n",
    "    # Initialize the custom Sigmoid activation function\n",
    "    custom_sigmoid = Sigmoid()\n",
    "\n",
    "    # Compute the activation using the custom Sigmoid class\n",
    "    activated_x_custom = custom_sigmoid(x)\n",
    "\n",
    "    # Perform a backward pass to compute gradients for the custom implementation\n",
    "    gradients_custom = custom_sigmoid.backward(torch.ones_like(activated_x_custom))\n",
    "\n",
    "    # Print the outputs and gradients from the custom implementation\n",
    "    print(\"Custom Sigmoid output:\", activated_x_custom)\n",
    "    print(\"Custom Sigmoid gradients:\", gradients_custom)\n",
    "    \n",
    "    # Reset gradients to zero before another backward pass\n",
    "    x.grad = None\n",
    "    \n",
    "    # Compute the activation using PyTorch's built-in sigmoid function\n",
    "    activated_x_torch = torch.sigmoid(x)\n",
    "\n",
    "    # Perform a backward pass to compute gradients for PyTorch's implementation\n",
    "    activated_x_custom.backward(torch.ones_like(x))\n",
    "    gradients_torch = x.grad\n",
    "\n",
    "    # Print the outputs and gradients from PyTorch's implementation\n",
    "    print(\"PyTorch Sigmoid output:\", activated_x_torch)\n",
    "    print(\"PyTorch Sigmoid gradients:\", gradients_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "RDdQGqu2di62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Tanh output: tensor([-0.7616,  0.0000,  0.7616,  0.9640], grad_fn=<DivBackward0>)\n",
      "Custom Tanh gradients: tensor([0.4200, 1.0000, 0.4200, 0.0707], grad_fn=<MulBackward0>)\n",
      "PyTorch Tanh output: tensor([-0.7616,  0.0000,  0.7616,  0.9640], grad_fn=<TanhBackward0>)\n",
      "PyTorch Tanh gradients: tensor([0.4200, 1.0000, 0.4200, 0.0707])\n"
     ]
    }
   ],
   "source": [
    "class Tanh(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the Tanh activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for Tanh.\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor where Tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)).\n",
    "        \"\"\"\n",
    "        self.output = (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
    "        return self.output\n",
    "    \n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for custom Tanh.\n",
    "        :param grad_output: Gradient tensor of the output.\n",
    "        :return: Gradient tensor for the input.\n",
    "        \"\"\"\n",
    "        # Gradient of tanh function\n",
    "        grad_input = (1 - self.output ** 2) * grad_output\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a sample input tensor\n",
    "    x = torch.tensor([-1.0, 0.0, 1.0, 2.0], requires_grad=True)\n",
    "\n",
    "    # Initialize the custom Tanh activation function\n",
    "    custom_tanh = Tanh()\n",
    "\n",
    "    # Compute the activation using the custom Tanh class\n",
    "    activated_x_custom = custom_tanh(x)\n",
    "\n",
    "    # Perform a backward pass to compute gradients for the custom implementation\n",
    "    gradients_custom = custom_tanh.backward(torch.ones_like(activated_x_custom))\n",
    "\n",
    "    # Print the outputs and gradients from the custom implementation\n",
    "    print(\"Custom Tanh output:\", activated_x_custom)\n",
    "    print(\"Custom Tanh gradients:\", gradients_custom)\n",
    "    \n",
    "    # Reset gradients to zero before another backward pass\n",
    "    x.grad = None\n",
    "    \n",
    "    # Compute the activation using PyTorch's built-in Tanh function\n",
    "    activated_x_torch = torch.tanh(x)\n",
    "\n",
    "    # Perform a backward pass to compute gradients for PyTorch's implementation\n",
    "    activated_x_custom.backward(torch.ones_like(x))\n",
    "    gradients_torch = x.grad\n",
    "\n",
    "    # Print the outputs and gradients from PyTorch's implementation\n",
    "    print(\"PyTorch Tanh output:\", activated_x_torch)\n",
    "    print(\"PyTorch Tanh gradients:\", gradients_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GHGtfdCC3jw9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Softmax output: tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]], grad_fn=<DivBackward0>)\n",
      "PyTorch Softmax output: tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]], grad_fn=<SoftmaxBackward0>)\n",
      "PyTorch Softmax gradients: tensor([[-8.0666e-09, -2.1927e-08,  2.9994e-08],\n",
      "        [-8.0666e-09, -2.1927e-08,  2.9994e-08]])\n"
     ]
    }
   ],
   "source": [
    "class Softmax(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the Softmax activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "\n",
    "    def forward(self, x, dim=1):\n",
    "        \"\"\"\n",
    "        Forward pass for Softmax.\n",
    "        :param x: Input tensor.\n",
    "        :param dim: The dimension Softmax would be applied to.\n",
    "        :return: Output tensor after applying Softmax.\n",
    "        \"\"\"\n",
    "        # Subtract the maximum value in each row for numerical stability\n",
    "        max_vals, _ = torch.max(x, dim=dim, keepdim=True)\n",
    "        exp_x = torch.exp(x - max_vals)\n",
    "        softmax_output = exp_x / torch.sum(exp_x, dim=dim, keepdim=True)\n",
    "        return softmax_output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a sample input tensor\n",
    "    x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                      [4.0, 5.0, 6.0]], requires_grad=True)\n",
    "\n",
    "    # Initialize the custom Softmax activation function\n",
    "    custom_softmax = Softmax()\n",
    "\n",
    "    # Compute the activation using the custom Softmax class\n",
    "    activated_x_custom = custom_softmax(x)\n",
    "\n",
    "    # Print the outputs and gradients from the custom implementation\n",
    "    print(\"Custom Softmax output:\", activated_x_custom)\n",
    "    \n",
    "    # Compute the activation using PyTorch's built-in Softmax function\n",
    "    activated_x_torch = torch.softmax(x, dim=1)\n",
    "    \n",
    "    # Perform a backward pass to compute gradients for PyTorch's implementation\n",
    "    activated_x_custom.backward(torch.ones_like(x))\n",
    "    gradients_torch = x.grad\n",
    "    \n",
    "    # Print the outputs and gradients from PyTorch's implementation\n",
    "    print(\"PyTorch Softmax output:\", activated_x_torch)\n",
    "    print(\"PyTorch Softmax gradients:\", gradients_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDE6cU-EmcC0",
    "tags": []
   },
   "source": [
    "# Task: Connecting Sigmoid and Softmax Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-um4CM3Kxwh"
   },
   "source": [
    "The sigmoid and softmax functions are foundational to machine learning, particularly in classification tasks. While the sigmoid function is traditionally used for binary classification, the softmax function generalizes this concept to multi-class problems. The sigmoid function can be seen as a special case of the softmax function when the output space consists of two classes.\n",
    "\n",
    "Consider a binary classification problem and the general form of the softmax function for an arbitrary vector $\\mathbf{z} $ with components $\\mathbf{z_i} $ for $\\mathbf( i = 1, \\ldots, K) $ classes. The softmax function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "The task is to demonstrate that the softmax function simplifies to the sigmoid function in the context of binary classification.\n",
    "\n",
    "**Express the Softmax Function for Two Classes:**\n",
    "Show the softmax function for a two-class system and define the components of the vector $\\mathbf{z} $ as arbitrary logits without specifying any particular values.\n",
    "\n",
    "For a two-class system, let $\\mathbf{z} = [z_1, z_2]$. Then the softmax function becomes:\n",
    "$$\\text{softmax}(\\mathbf{z_1}) = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}$$\n",
    "$$\\text{softmax}(\\mathbf{z_2}) = \\frac{e^{z_2}}{e^{z_1} + e^{z_2}}$$\n",
    "\n",
    "\n",
    "**Derive the Sigmoid Function from Softmax:**\n",
    "Simplify the expression for the probability of the first class and show how it is equivalent to the sigmoid function for an arbitrary logit.\n",
    "\n",
    "For binary classification, if we denote $z_1$ as the logit for the positive class (let's say class 1), then the logit for the negative class (class 2) can be expressed in terms of $z_1$. Since the probabilities must sum to one, we have:\n",
    "\n",
    "$$ P(y=1|\\mathbf{z}) + P(y=2|\\mathbf{z}) = 1 $$\n",
    "\n",
    "Given that there are only two classes, $P(y=2|\\mathbf{z})$ represents the probability of the negative class, which we can express in terms of $P(y=1|\\mathbf{z})$. Therefore, we can set $z_2 = 0$ without loss of generality, because $e^0 = 1$:\n",
    "\n",
    "$$ P(y=1|\\mathbf{z}) = \\frac{e^{z}}{e^{z} + e^{0}} = \\frac{e^{z}}{e^{z} + 1} $$\n",
    "\n",
    "So, $z_1 = z$ and $z_2 = 0$.\n",
    "\n",
    "Finally, dividing by $e^{z}$ gives the sigmoid function:\n",
    "\n",
    "$$ = \\frac{1}{1 + e^{-z}} = \\text{sigmoid}(\\mathbf{z}) $$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
